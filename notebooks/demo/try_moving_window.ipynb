{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Averaged action = 1.0\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class RollingWindowPolicy:\n",
    "    def __init__(self, h):\n",
    "        self.h = h  # Horizon of action steps\n",
    "        self.policies = deque()  # Initialize a deque to store the policies\n",
    "\n",
    "    def add_policy(self, policy):\n",
    "        \"\"\"\n",
    "        Add a new policy to the deque. If the deque already contains `h` policies,\n",
    "        remove the oldest one to maintain the window size.\n",
    "        \"\"\"\n",
    "        if len(self.policies) == self.h:\n",
    "            self.policies.popleft()  # Remove the oldest policy if we hit the limit\n",
    "        self.policies.append(policy)\n",
    "\n",
    "    def get_averaged_action(self, iteration):\n",
    "        \"\"\"\n",
    "        Calculate the averaged action for the given iteration.\n",
    "        The iteration starts at 0 and signifies the shifting window position.\n",
    "        \"\"\"\n",
    "        if not self.policies:\n",
    "            raise ValueError(\"No policies have been added yet.\")\n",
    "        \n",
    "        # Initialize variables to calculate the sum and count for averaging\n",
    "        action_sum = 0\n",
    "        count = 0\n",
    "\n",
    "        # Loop over the policies and accumulate actions for the iteration\n",
    "        for i in range(min(iteration + 1, len(self.policies))):\n",
    "            action_sum += self.policies[i][iteration - i]  # Adjust index for overlap\n",
    "            count += 1\n",
    "\n",
    "        # Calculate the average action for this iteration\n",
    "        return action_sum / count if count > 0 else None\n",
    "\n",
    "# Example usage\n",
    "h = 6  # Define the horizon\n",
    "rp = RollingWindowPolicy(h)\n",
    "\n",
    "# Adding policies\n",
    "policies = [\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [2, 3, 4, 5, 6],\n",
    "    [3, 4, 5, 6, 7],\n",
    "    [4, 5, 6, 7, 8],\n",
    "    [5, 6, 7, 8, 9],\n",
    "    [6, 7, 8, 9, 10],\n",
    "]\n",
    "\n",
    "for policy in policies:\n",
    "    rp.add_policy(policy)\n",
    "\n",
    "# Calculating averaged actions for the first few iterations\n",
    "iteration = 0\n",
    "averaged_action = rp.get_averaged_action(iteration)\n",
    "print(f\"Iteration {iteration + 1}: Averaged action = {averaged_action}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged policy for the next step: [2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AveragedRollingPolicy:\n",
    "    def __init__(self, h):\n",
    "        self.h = h\n",
    "        self.policies = []\n",
    "\n",
    "    def add_policy(self, new_policy):\n",
    "        \"\"\"\n",
    "        Add a new policy and remove the oldest if we exceed the window size.\n",
    "        \"\"\"\n",
    "        if len(self.policies) >= self.h:\n",
    "            self.policies.pop(0)\n",
    "        self.policies.append(new_policy)\n",
    "\n",
    "    def calculate_next_step_policy(self):\n",
    "        \"\"\"\n",
    "        Calculate the policy for the next step as an average of overlapping actions\n",
    "        from all policies in the window.\n",
    "        \"\"\"\n",
    "        if not self.policies:\n",
    "            raise ValueError(\"No policies available to calculate the next step.\")\n",
    "        \n",
    "        # Initialize an array to accumulate actions for the next step\n",
    "        next_step_actions = np.zeros(self.h)\n",
    "        count_actions = np.zeros(self.h)\n",
    "        \n",
    "        # Loop through the policies and their steps\n",
    "        for i, policy in enumerate(self.policies):\n",
    "            for j, action in enumerate(policy):\n",
    "                # For each policy, the relevant action for the next step shifts\n",
    "                next_step_actions[j] += action\n",
    "                count_actions[j] += 1\n",
    "        \n",
    "        # Calculate the average actions for each step in the next policy\n",
    "        averaged_policy = next_step_actions / count_actions\n",
    "        return averaged_policy\n",
    "\n",
    "# Example usage\n",
    "h = 3  # Define the horizon\n",
    "arp = AveragedRollingPolicy(h)\n",
    "\n",
    "# Simulate adding policies over time\n",
    "policies = [\n",
    "    [1, 2, 3],\n",
    "    [2, 3, 4],\n",
    "    [3, 4, 5]\n",
    "]\n",
    "\n",
    "for policy in policies:\n",
    "    arp.add_policy(policy)\n",
    "\n",
    "# Calculate the averaged policy for the next step\n",
    "next_step_policy = arp.calculate_next_step_policy()\n",
    "print(f\"Averaged policy for the next step: {next_step_policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ngboost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
